{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este codigo fue desarrollado por Oscar Pérez, tutor de EBAC.\n",
    "Te invito a seguirme en instagram @Oscaruph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oraciones: ['\"Ten miedo, pero hazlo de todos modos.', 'Lo importante es la acción.', 'No tienes que esperar\\n a tener confianza.', 'Simplemente hazlo y, con el tiempo, la confianza te seguirá\".', '- Carrie Fisher.']\n",
      "Tokens: ['``', 'Ten', 'miedo', ',', 'pero', 'hazlo', 'de', 'todos', 'modos', '.', 'Lo', 'importante', 'es', 'la', 'acción', '.', 'No', 'tienes', 'que', 'esperar', 'a', 'tener', 'confianza', '.', 'Simplemente', 'hazlo', 'y', ',', 'con', 'el', 'tiempo', ',', 'la', 'confianza', 'te', 'seguirá', \"''\", '.', '-', 'Carrie', 'Fisher', '.']\n",
      "Etiquetas gramaticales: [('``', '``'), ('Ten', 'CD'), ('miedo', 'NN'), (',', ','), ('pero', 'NN'), ('hazlo', 'NN'), ('de', 'IN'), ('todos', 'FW'), ('modos', 'NN'), ('.', '.'), ('Lo', 'NNP'), ('importante', 'JJ'), ('es', 'NN'), ('la', 'NN'), ('acción', 'NN'), ('.', '.'), ('No', 'DT'), ('tienes', 'NNS'), ('que', 'RB'), ('esperar', 'VBP'), ('a', 'DT'), ('tener', 'NN'), ('confianza', 'NN'), ('.', '.'), ('Simplemente', 'NNP'), ('hazlo', 'NN'), ('y', 'NN'), (',', ','), ('con', 'NN'), ('el', 'NN'), ('tiempo', 'NN'), (',', ','), ('la', 'FW'), ('confianza', 'FW'), ('te', 'FW'), ('seguirá', 'NN'), (\"''\", \"''\"), ('.', '.'), ('-', ':'), ('Carrie', 'NN'), ('Fisher', 'NNP'), ('.', '.')]\n",
      "Entidades nombradas: (S\n",
      "  ``/``\n",
      "  Ten/CD\n",
      "  miedo/NN\n",
      "  ,/,\n",
      "  pero/NN\n",
      "  hazlo/NN\n",
      "  de/IN\n",
      "  todos/FW\n",
      "  modos/NN\n",
      "  ./.\n",
      "  Lo/NNP\n",
      "  importante/JJ\n",
      "  es/NN\n",
      "  la/NN\n",
      "  acción/NN\n",
      "  ./.\n",
      "  No/DT\n",
      "  tienes/NNS\n",
      "  que/RB\n",
      "  esperar/VBP\n",
      "  a/DT\n",
      "  tener/NN\n",
      "  confianza/NN\n",
      "  ./.\n",
      "  Simplemente/NNP\n",
      "  hazlo/NN\n",
      "  y/NN\n",
      "  ,/,\n",
      "  con/NN\n",
      "  el/NN\n",
      "  tiempo/NN\n",
      "  ,/,\n",
      "  la/FW\n",
      "  confianza/FW\n",
      "  te/FW\n",
      "  seguirá/NN\n",
      "  ''/''\n",
      "  ./.\n",
      "  -/:\n",
      "  (PERSON Carrie/NN Fisher/NNP)\n",
      "  ./.)\n",
      "Palabras filtradas: ['``', 'Ten', 'miedo', ',', 'pero', 'hazlo', 'de', 'todos', 'modos', '.', 'Lo', 'importante', 'es', 'la', 'acción', '.', 'tienes', 'que', 'esperar', 'tener', 'confianza', '.', 'Simplemente', 'hazlo', ',', 'con', 'el', 'tiempo', ',', 'la', 'confianza', 'te', 'seguirá', \"''\", '.', '-', 'Carrie', 'Fisher', '.']\n"
     ]
    }
   ],
   "source": [
    "def process_text(text):\n",
    "    # Tokenización de oraciones\n",
    "    sentences = sent_tokenize(text)\n",
    "    print(\"Oraciones:\", sentences)\n",
    "\n",
    "    # Tokenización de palabras\n",
    "    tokens = word_tokenize(text)\n",
    "    print(\"Tokens:\", tokens)\n",
    "\n",
    "    # Etiquetado gramatical\n",
    "    tagged = pos_tag(tokens)\n",
    "    print(\"Etiquetas gramaticales:\", tagged)\n",
    "\n",
    "    # Reconocimiento de entidades nombradas\n",
    "    named_entities = ne_chunk(tagged)\n",
    "    print(\"Entidades nombradas:\", named_entities)\n",
    "\n",
    "    # Eliminación de palabras vacías (stop words)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [token for token in tokens if token.lower() not in stop_words]\n",
    "    print(\"Palabras filtradas:\", filtered_words)\n",
    "\n",
    "# Ruta del archivo de texto\n",
    "ruta_archivo = \"./preubaLEERTEXTO.txt\"  # Reemplaza \"ruta_del_archivo.txt\" con la ruta real de tu archivo\n",
    "\n",
    "# Leer contenido del archivo\n",
    "with open(ruta_archivo, 'r') as archivo:\n",
    "    contenido = archivo.read()\n",
    "\n",
    "# Procesar el texto\n",
    "process_text(contenido)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
